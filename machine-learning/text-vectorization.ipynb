{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization Pipeline\n",
    "\n",
    "This example illustrates how Dask-ML can be used to vectorize large textual datasets in parallel.\n",
    "This example is adapted from [this scikit-learn example](https://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py).\n",
    "\n",
    "It's addapted to\n",
    "\n",
    "* Fit the entire model, including text vectorization, as a pipeline.\n",
    "* Use dask collections like `dask.bag`, `dask.dataframe`, and `dask.array`\n",
    "  rather than generators to work with larger than memory datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:64775</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>2.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:64775' processes=2 threads=4, memory=2.00 GB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='1GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data\n",
    "\n",
    "The data are available on the [UCI machine learning repository](https://archive.ics.uci.edu/ml/index.php).\n",
    "The details of downloading and parsing the data aren't too important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "from glob import glob\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import scipy.sparse\n",
    "from sklearn.datasets import get_data_home\n",
    "\n",
    "import dask\n",
    "import dask.bag as db\n",
    "\n",
    "\n",
    "import dask_ml.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reuters(data_path=None):\n",
    "    \"\"\"Fetch documents of the Reuters dataset\"\"\"\n",
    "    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "                    'reuters21578-mld/reuters21578.tar.gz')\n",
    "    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n",
    "\n",
    "    if data_path is None:\n",
    "        data_path = os.path.join(get_data_home(), \"reuters\")\n",
    "    if not os.path.exists(data_path):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        print(\"downloading dataset (once and for all) into %s\" %\n",
    "              data_path)\n",
    "        os.mkdir(data_path)\n",
    "\n",
    "        def progress(blocknum, bs, size):\n",
    "            if blocknum % 100 == 0:\n",
    "                total_sz_mb = '%.2f MB' % (size / 1e6)\n",
    "                current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n",
    "                print('\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb))\n",
    "\n",
    "        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n",
    "        urlretrieve(DOWNLOAD_URL, filename=archive_path,\n",
    "                    reporthook=progress)\n",
    "        print('\\r')\n",
    "        print(\"untarring Reuters dataset...\")\n",
    "        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n",
    "        print(\"done.\")\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def load_from_filename(file_path):\n",
    "    \"\"\"\n",
    "    Load a list of (topic, article) pairs.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> pairs = load_from_file(\"reut2-004.sgm\")\n",
    "    >>> topics, article = pairs[0]\n",
    "    >>> topics\n",
    "    ['interest', 'retail', 'ipi']\n",
    "    >>> article[:37]\n",
    "    'U.S. economic data this week could be'\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as fh:\n",
    "        txt = fh.read().decode('latin-1')        \n",
    "\n",
    "    *articles, _ = [x.strip() for x in txt.split(\"</REUTERS>\")]\n",
    "    articles = [x for x in articles\n",
    "                if '<BODY>' in x\n",
    "                and '<TOPICS></TOPICS>' not in x]\n",
    "    articles = '\\n'.join(articles)\n",
    "    topics = re.findall('<TOPICS>(?P<topics>.*)<\\/TOPICS>', articles)\n",
    "    topics = [re.findall('(?<=<D>)[^<]+(?=</D>)', x) for x in topics]\n",
    "    bodies = re.findall('(?<=<BODY>)[^<]+(?=</BODY>)', articles)\n",
    "\n",
    "    return list(zip(topics, bodies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/taugspurger/scikit_learn_data/reuters/reut2-004.sgm',\n",
       " '/Users/taugspurger/scikit_learn_data/reuters/reut2-010.sgm',\n",
       " '/Users/taugspurger/scikit_learn_data/reuters/reut2-011.sgm',\n",
       " '/Users/taugspurger/scikit_learn_data/reuters/reut2-005.sgm',\n",
       " '/Users/taugspurger/scikit_learn_data/reuters/reut2-013.sgm']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = fetch_reuters()\n",
    "files = glob(os.path.join(data_path, 'reut2*'))\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`files` is a list of filepaths. We can build a Dask Bag that will (lazily)\n",
    "read in the contents of these files. Again, the details of loading aren't\n",
    "the point of this example, but you may want to glance through `load_from_filename`\n",
    "to see an example of `dask.bag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[interest, retail, ipi]</td>\n",
       "      <td>U.S. economic data this week could be\\nthe key...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[earn]</td>\n",
       "      <td>Oper shr loss two cts vs profit three cts\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[earn]</td>\n",
       "      <td>Shr 25 cts vs 36 cts\\n    Net 1.4 mln vs 1.4 m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[earn]</td>\n",
       "      <td>Shr loss 1.02 dlrs vs 1.01 dlr\\n    Net loss 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[crude, nat-gas, iron-steel]</td>\n",
       "      <td>USX Corp said proved reserves of oil\\nand natu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         topics  \\\n",
       "0       [interest, retail, ipi]   \n",
       "1                        [earn]   \n",
       "2                        [earn]   \n",
       "3                        [earn]   \n",
       "4  [crude, nat-gas, iron-steel]   \n",
       "\n",
       "                                                text  \n",
       "0  U.S. economic data this week could be\\nthe key...  \n",
       "1  Oper shr loss two cts vs profit three cts\\n   ...  \n",
       "2  Shr 25 cts vs 36 cts\\n    Net 1.4 mln vs 1.4 m...  \n",
       "3  Shr loss 1.02 dlrs vs 1.01 dlr\\n    Net loss 1...  \n",
       "4  USX Corp said proved reserves of oil\\nand natu...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = (\n",
    "    db.from_sequence(files)\n",
    "      .map(load_from_filename)\n",
    "      .flatten()\n",
    "      .to_dataframe(columns=['topics', 'text'])\n",
    ")\n",
    "\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`text` is a dask dataframe with two columns.\n",
    "\n",
    "* topics: A list of topics this article is classified with\n",
    "* text: the body of the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dask_ml.feature_extraction.text.HashingVectorizer` provides a similar API to [scikit-learn's implementation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html). In fact, Dask-ML's implementation uses scikit-learn's, applying it to each partition of the input `dask.dataframe.Series` or `dask.bag.Bag`.\n",
    "\n",
    "Transformation, once we actually compute the result, happens in parallel and returns a dask Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> unknown </td> <td> unknown </td></tr>\n",
       "    <tr><th> Shape </th><td> (nan, 1048576) </td> <td> (nan, 1048576) </td></tr>\n",
       "    <tr><th> Count </th><td> 88 Tasks </td><td> 22 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<_transform, shape=(nan, 1048576), dtype=float64, chunksize=(nan, 1048576), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = dask_ml.feature_extraction.text.HashingVectorizer()\n",
    "X = vect.fit_transform(text['text'])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output array has unknown chunk sizes becase dask Series and Bags don't know their own length.\n",
    "If you need those, as some `dask.array` operations do, use `X.compute_chunk_sizes()` to get them (at the cost of some computation).\n",
    "\n",
    "\n",
    "Each block in `X` is a `scipy.sparse` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<505x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 39630 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.blocks[0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a document-term matrix. Each row is the hashed representation of the original article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Pipeline\n",
    "\n",
    "We can combine the `HashingVectorizer` with `Incremental` and a classifier like `SGDClassifier` to\n",
    "create a classification pipeline.\n",
    "\n",
    "We'll predict whether the document is assigned the `acq` topic. Recall that each row in the\n",
    "Series is a list of topics, so `acq` in `x` applied to each row will give us the indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 83.02 kB </td> <td> 4.57 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (10377,) </td> <td> (571,) </td></tr>\n",
       "    <tr><th> Count </th><td> 110 Tasks </td><td> 22 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> int64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"5\" y1=\"0\" x2=\"5\" y2=\"25\" />\n",
       "  <line x1=\"11\" y1=\"0\" x2=\"11\" y2=\"25\" />\n",
       "  <line x1=\"18\" y1=\"0\" x2=\"18\" y2=\"25\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"25\" />\n",
       "  <line x1=\"27\" y1=\"0\" x2=\"27\" y2=\"25\" />\n",
       "  <line x1=\"33\" y1=\"0\" x2=\"33\" y2=\"25\" />\n",
       "  <line x1=\"39\" y1=\"0\" x2=\"39\" y2=\"25\" />\n",
       "  <line x1=\"46\" y1=\"0\" x2=\"46\" y2=\"25\" />\n",
       "  <line x1=\"52\" y1=\"0\" x2=\"52\" y2=\"25\" />\n",
       "  <line x1=\"58\" y1=\"0\" x2=\"58\" y2=\"25\" />\n",
       "  <line x1=\"65\" y1=\"0\" x2=\"65\" y2=\"25\" />\n",
       "  <line x1=\"71\" y1=\"0\" x2=\"71\" y2=\"25\" />\n",
       "  <line x1=\"77\" y1=\"0\" x2=\"77\" y2=\"25\" />\n",
       "  <line x1=\"83\" y1=\"0\" x2=\"83\" y2=\"25\" />\n",
       "  <line x1=\"85\" y1=\"0\" x2=\"85\" y2=\"25\" />\n",
       "  <line x1=\"91\" y1=\"0\" x2=\"91\" y2=\"25\" />\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"95\" y2=\"25\" />\n",
       "  <line x1=\"99\" y1=\"0\" x2=\"99\" y2=\"25\" />\n",
       "  <line x1=\"104\" y1=\"0\" x2=\"104\" y2=\"25\" />\n",
       "  <line x1=\"110\" y1=\"0\" x2=\"110\" y2=\"25\" />\n",
       "  <line x1=\"116\" y1=\"0\" x2=\"116\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 120.000000,0.000000 120.000000,25.412617 0.000000,25.412617\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >10377</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<values, shape=(10377,), dtype=int64, chunksize=(571,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = text.topics.apply(\n",
    "    lambda x: int('acq' in x), meta=('topics', 'int')\n",
    ")\n",
    "y.to_dask_array(lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask_ml.metrics\n",
    "from dask_ml.wrappers import Incremental\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the input comes from a dask Series, with unknown chunk sizes, we need to specify `assume_equal_chunks=True`. This tells Dask-ML that we know that each partition in `X`\n",
    "matches a partition in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(max_iter=5, tol=1e-3)\n",
    "clf = Incremental(sgd, scoring='accuracy', assume_equal_chunks=True)\n",
    "pipe = make_pipeline(vect, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('hashingvectorizer',\n",
       "                 HashingVectorizer(alternate_sign=True, analyzer='word',\n",
       "                                   binary=False, decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, n_features=1048576,\n",
       "                                   ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=None, stop_words=None,\n",
       "                                   strip_accents=None,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   t...\n",
       "                                                     class_weight=None,\n",
       "                                                     early_stopping=False,\n",
       "                                                     epsilon=0.1, eta0=0.0,\n",
       "                                                     fit_intercept=True,\n",
       "                                                     l1_ratio=0.15,\n",
       "                                                     learning_rate='optimal',\n",
       "                                                     loss='hinge', max_iter=5,\n",
       "                                                     n_iter_no_change=5,\n",
       "                                                     n_jobs=None, penalty='l2',\n",
       "                                                     power_t=0.5,\n",
       "                                                     random_state=None,\n",
       "                                                     shuffle=True, tol=0.001,\n",
       "                                                     validation_fraction=0.1,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False),\n",
       "                             random_state=None, scoring='accuracy',\n",
       "                             shuffle_blocks=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(text['text'], y, incremental__classes=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, `Incremental.predict` lazily returns the predictions as a dask Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> unknown </td> <td> unknown </td></tr>\n",
       "    <tr><th> Shape </th><td> (nan,) </td> <td> (nan,) </td></tr>\n",
       "    <tr><th> Count </th><td> 110 Tasks </td><td> 22 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> int64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<_predict, shape=(nan,), dtype=int64, chunksize=(nan,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipe.predict(text['text'])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute them and the score in parallel with `dask_ml.metrics.accuracy_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9774501300954033"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_ml.metrics.accuracy_score(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple combination of a HashingVectorizer and SGDClassifier is\n",
    "pretty effective at this prediction task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
