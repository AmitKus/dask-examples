{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gotcha's from Pandas to Dask\n",
    "\n",
    "This notebook highlights some key differences when transfering code from Pandas to run in a Dask environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Dask Client for Dashboard\n",
    "\n",
    "Starting the Dask Client is optional.  In this example we are running on a `LocalCluster`, this  will provide a dashboard which is useful to gain insight on the computation.  \n",
    "\n",
    "The link to the dashboard will become visible when you create a client (as shown below). When running in `Jupyter Lab` an [extenstion](https://github.com/dask/dask-labextension) can be installed to be able to view the various dashboard widgets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:63296\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>8.50 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:63296' processes=4 cores=4>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "# client = Client(n_workers=1, threads_per_worker=4, processes=False, memory_limit='2GB')\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [documentation for addtional cluster configuration](http://distributed.dask.org/en/latest/local-cluster.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask versoin: 1.2.2\n",
      "Pandas versoin: 0.24.2\n"
     ]
    }
   ],
   "source": [
    "# since Dask activly beeing developed - the current example is running with the below version\n",
    "import dask\n",
    "import pandas as pd\n",
    "print(f'Dask versoin: {dask.__version__}')\n",
    "print(f'Pandas versoin: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 2 DataFrames for comparison: \n",
    "1. for Dask \n",
    "2. for Pandas  \n",
    "Dask comes with builtin dataset samples, we will use this sample for our example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask.datasets.timeseries()\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remember `Dask framework` is **lazy** thus in order to see the result we need to run [compute()](https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.compute) \n",
    " (or `head()` which runs under the hood compute()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a `Pandas` dataframe we can use the `compute()` method from a `Dask dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = ddf.compute()  \n",
    "print(type(pdf))\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `Dask dataframe` from Pandas\n",
    "In order to create a `Dask dataframe` (ddf) from a `Pandas dataframe` (pdf) we can use the [from_pandas](https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.from_pandas) method. \n",
    "You must supply the number of partitions or chunksize that will be used to generate the dask dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2 = dask.dataframe.from_pandas(pdf, npartitions=10)\n",
    "ddf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both `dataframes` we can start to compair the interactions with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual shift - from Update to Insert/Delete\n",
    "Dask does not update - thus there are no arguments such as `inplace=True` which exist in Pandas.  \n",
    "For more detials see [issue#653 on github](https://github.com/dask/dask/issues/653)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "print(pdf.columns)\n",
    "pdf.rename(columns={'id':'ID','name':'Name','x':'coor_x', 'y':'coor_y'},inplace=True)\n",
    "pdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask - Error\n",
    "ddf.rename(columns={'id':'ID','name':'Name','x':'coor_x', 'y':'coor_y'}, inplace=True)\n",
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask\n",
    "# Must update using the correct sequence of the columns\n",
    "print(ddf.columns)\n",
    "ddf = ddf.rename(columns={'id':'ID','name':'Name','x':'coor_x', 'y':'coor_y'})\n",
    "# or ddf.columns = ['ID','Name','coor_x','coor_y']\n",
    "ddf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data munipilations  \n",
    "There are several diffrences when manipulating data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loc - Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = (pdf['coor_x']>0.5) &(pdf['coor_x']<0.8)\n",
    "pdf[cond].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.loc[cond, ['coor_x']] = pdf['coor_x']* 10\n",
    "pdf[cond].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask - use mask/where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_dask = (ddf['coor_x']>0.5) & (ddf['coor_x']<0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error\n",
    "ddf.loc[cond_dask, ['coor_x']] = ddf['coor_x']* 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* remember that each dataframe partition can have duplicate indecies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['coor_x'] = ddf['coor_x'].mask(cond_dask, ddf['coor_x']* 10)\n",
    "ddf[cond_dask].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta\n",
    "One key difference is the introduction of `meta`.  \n",
    "> `meta` is the prescription of the names/types of the output from the computation  \n",
    "[see stack overflow answer](https://stackoverflow.com/questions/44432868/dask-dataframe-apply-meta)\n",
    "\n",
    "Since Dask creates a DAG for the computation it requires to understand what are the outputs of each calculation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['initials'] = pdf.Name.apply(lambda x: x[0]+x[1])\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['initials'] = ddf.Name.apply(lambda x: x[0]+x[1])\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['initials'] = ddf.Name.apply(lambda x: x[0]+x[1], meta = pd.Series(object, name='initials'))\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:00</th>\n",
       "      <td>941</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>0.323091</td>\n",
       "      <td>0.758048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:01</th>\n",
       "      <td>1042</td>\n",
       "      <td>Quinn</td>\n",
       "      <td>0.876541</td>\n",
       "      <td>-0.366250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id     name         x         y\n",
       "timestamp                                             \n",
       "2000-01-01 00:00:00   941  Charlie  0.323091  0.758048\n",
       "2000-01-01 00:00:01  1042    Quinn  0.876541 -0.366250"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = dask.datasets.timeseries()\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(row):\n",
    "    if (row['x']> 0):\n",
    "        return row['x'] * 1000  \n",
    "    else:\n",
    "        return row['y'] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf['z'] = ddf.apply(func, args=('coor_x', 'coor_y'), axis=1, meta=('z', 'float'))\n",
    "ddf['z'] = ddf.apply(func, axis=1, meta=('z', 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(df, col1, col2):\n",
    "#     if row[col1]< 0:\n",
    "#         return row[col1] * -1  \n",
    "#     else:\n",
    "#         return row[col2] * 10000\n",
    "        return df[col1] * df[col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['z'] = ddf.map_partitions(func2, 'x', 'y', meta=('z', 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj, transform\n",
    "inProj = Proj(init='epsg:3857')\n",
    "outProj = Proj(init='epsg:4326')\n",
    "def coor_trans(row, x1, y1):\n",
    "    x2,y2 = transform(inProj,outProj,row[x1],row[y1])\n",
    "    return x2,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.map_partitions?\n",
    "    data['LatLong'] = \n",
    "    data.apply(lambda row:  transform(Proj(init=row['EPSG']),Proj(init='epsg:4326'),row['X'],row['Y']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.apply(lambda row:  transform(Proj(init='epsg:3857'),Proj(init='epsg:4326'),row['x'],row['y']), axis=1, meta={'x2':float,'y2':float}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = ddf.apply(coor_trans, 'x','y', axis=1, meta={'x2':float,'y2':float} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert index into Time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "pdf = pdf.assign(Time=pd.to_datetime(pdf.index).time)\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf = ddf.assign(Time= dask.dataframe.to_datetime(ddf.index, format='%Y-%m-%d'). )\n",
    "ddf = ddf.assign(Time= dask.dataframe.to_datetime(ddf.index).dt.time )\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask or Pandas\n",
    "ddf = ddf.assign(Time2=ddf.index)\n",
    "ddf['Time2'] = ddf['Time2'].dt.time\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop NA on column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "pdf = pdf.assign(colna = None)\n",
    "print(pdf.head())\n",
    "pdf.dropna(axis=1, how='all', inplace=True)\n",
    "print(pdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask\n",
    "ddf = ddf.assign(colna = None)\n",
    "print(ddf.head())\n",
    "if ddf.colna.isnull().all().compute() == True:   # check if all values in column are Null - VERY slow\n",
    "    ddf = ddf.drop(labels=['colna'],axis=1)\n",
    "print(ddf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.4 Reset Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "pdf.reset_index(drop=True, inplace=True)\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is in a development mode\n",
    "thus there bugs that are fixed all the time   \n",
    "e.g. [reset_index fails when index is named ](https://github.com/dask/dask/pull/4509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This currently fails without reseting the dataframe......\n",
    "ddf = dask.datasets.timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask\n",
    "ddf.index.name = None   # workaround\n",
    "ddf = ddf.reset_index()\n",
    "ddf['Time'] = ddf['timestamp'].dt.time\n",
    "ddf = ddf.drop(labels=['timestamp'], axis=1 )\n",
    "ddf.columns = ['ID','Name','coor_x','coor_y','Time']\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reads/Save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with pandas and dask preferable try and work with parquet.  \n",
    "Even so when working with Dask - the files can be read with multiple workers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "!mkdir data\n",
    "pdf.to_csv('data/pdf_single_file.csv')\n",
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask\n",
    "# 1. notice the '*' to allow for multiple file renaming. all kwrgs are applicable\n",
    "# 2. notice that the path to the directory may change based on the location of the running notebook\n",
    "ddf.to_csv('data/pd2dd/ddf*.csv', index = False)\n",
    "!ls data/pd2dd/\n",
    "# to fild number of partitions use dask.dataframe.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on an [answer from stack overflow](https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas \n",
    "path = r'data/'                     # use your path\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask\n",
    "ddf = dask.dataframe.read_csv('data/pd2dd/ddf*.csv')\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most `kwarg` are available for reading and writing \n",
    "e.g. \n",
    "ddf = dask.dataframe.read_csv('data/pd2dd/ddf*.csv', compression='gzip', header none)\n",
    "However `nrows` is not available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Group By\n",
    "In addition to the notebook example that is in the repository - \n",
    "This is another example how to try to eliminate the use of `groupby.apply`  \n",
    "In this example we are grouping by coloumns into unique list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare pandas dataframe\n",
    "pdf = pdf.assign(Time=pd.to_datetime(pdf.index).time)\n",
    "pdf['seconds'] = pdf.Time.astype(str).str[-2:]\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas preperations\n",
    "def set_list_att(x: dask.dataframe.Series):\n",
    "        return list(set([item for item in x.values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# pandas option 1 using apply\n",
    "pdf_gb = pdf.groupby(pdf.Name)\n",
    "gp_col = ['ID', 'seconds']\n",
    "list_ser_gb = [pdf_gb[att_col_gr].apply(set_list_att) for att_col_gr in gp_col]\n",
    "df_edge_att = pdf_gb.size().to_frame(name=\"Weight\")\n",
    "for ser in list_ser_gb:\n",
    "        df_edge_att = df_edge_att.join(ser.to_frame(), how='left')        \n",
    "df_edge_att.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# pandas option 2 using lambda\n",
    "pdf_gb = pdf.groupby(pdf.Name)\n",
    "gp_col = ['ID', 'seconds']\n",
    "list_ser_gb = [pdf_gb[att_col_gr].apply(lambda x: list(set(x.to_list()))) for att_col_gr in gp_col]\n",
    "df_edge_att = pdf_gb.size().to_frame(name=\"Weight\")\n",
    "for ser in list_ser_gb:\n",
    "        df_edge_att = df_edge_att.join(ser.to_frame(), how='left')        \n",
    "df_edge_att.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case sometimes using Pandas is more efficiante (assuming that you can load all the data into the RAM).  \n",
    "In this case Pandas is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dask dataframe\n",
    "ddf['seconds'] = ddf.Time.astype(str).str[-2:]\n",
    "ddf = client.persist(ddf)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Dask option1 using apply\n",
    "# notice the meta argument in the apply function\n",
    "df_gb = ddf.groupby(ddf.Name)\n",
    "gp_col = ['ID', 'seconds']\n",
    "list_ser_gb = [df_gb[att_col_gr].apply(set_list_att\n",
    "                                      ,meta=pd.Series(dtype='object', name=f'{att_col_gr}_att')) \n",
    "               for att_col_gr in gp_col]\n",
    "df_edge_att = df_gb.size().to_frame(name=\"Weight\")\n",
    "for ser in list_ser_gb:\n",
    "        df_edge_att = df_edge_att.join(ser.to_frame(), how='left')        \n",
    "df_edge_att.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [dask custom aggregation](https://docs.dask.org/en/latest/dataframe-api.html?highlight=dropna#dask.dataframe.groupby.Aggregation) is consideribly better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask\n",
    "# some preperations\n",
    "import itertools\n",
    "custom_agg = dask.dataframe.Aggregation(\n",
    "    'custom_agg', \n",
    "    lambda s: s.apply(set), \n",
    "    lambda s: s.apply(lambda chunks: list(set(itertools.chain.from_iterable(chunks)))),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Dask option1 using apply\n",
    "df_gb = ddf.groupby(ddf.Name)\n",
    "gp_col = ['ID', 'seconds']\n",
    "list_ser_gb = [df_gb[att_col_gr].agg(custom_agg) for att_col_gr in gp_col]\n",
    "df_edge_att = df_gb.size().to_frame(name=\"Weight\")\n",
    "for ser in list_ser_gb:\n",
    "        df_edge_att = df_edge_att.join(ser.to_frame(), how='left')        \n",
    "df_edge_att.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Consider using Persist\n",
    "Since Dask is lazy - it may ran the **entire** graph (again) even if it already ran part of it in order to generate a result \n",
    "in a previous cell.  \n",
    "```python\n",
    "ddf = client.persist(ddf)\n",
    "```\n",
    "This is different from Pandas which once a variable was created it will keep all data in memory.  \n",
    "Additional information can be read in this [stackoverflow issue](https://stackoverflow.com/questions/45941528/how-to-efficiently-send-a-large-numpy-array-to-the-cluster-with-dask-array/45941529#45941529) or see an exampel in [this post](http://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes)   \n",
    "This concept should also  be used when running a code within a script (rather then a jupyter notebook) which incoperates a loop logic within the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Debugging\n",
    "Debugging my be more challenging since\n",
    "1. when using a client - mutliprocessing is complecated\n",
    "2. sometime introducing a faulty command into a graph (such as in a jupyter notebook) requirues to cache-out the graph and start the process from the begining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "pipenvDask",
   "language": "python",
   "name": "pipenvdask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
