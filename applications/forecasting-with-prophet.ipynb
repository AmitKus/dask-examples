{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting\n",
    "\n",
    "This example shows using [Prophet](https://facebook.github.io/prophet/) and Dask for scalable time series forecasting.\n",
    "\n",
    "> Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.\n",
    "\n",
    "As discussed in the [*Forecasting at scale*](https://peerj.com/preprints/3190/), large datasets aren't the only type of scaling challenge teams run into. In this example we'll focus on one of the scaling challenges indentified in that paper:\n",
    "\n",
    "> in most realistic settings, a large number of forecasts will be created, necessitating efficient, automated means of evaluating and comparing them, as well as detecting when they are likely to be performing poorly. When hundreds or even thousands of forecasts are made, it becomes important to let machines do the hard work of model evaluation and comparison while efficiently using human feedback to fix performance problems.\n",
    "\n",
    "That sounds like a perfect opportunity for Dask. We'll use Prophet and Dask together to parallize the *diagnostics* stage of research. It does not attempt to parallize the training of the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example currently relies on Prophet master\n",
    "!pip install 'git+https://github.com/facebook/prophet/#egg=fbprophet&subdirectory=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll walk through the example from the Prophet quickstart. These values represent the log daily page views for [Peyton Manning's wikipedia page](https://en.wikipedia.org/wiki/Peyton_Manning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/facebook/prophet/master/examples/example_wp_log_peyton_manning.csv',\n",
    "    parse_dates=['ds']\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='ds', y='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model takes a handful of seconds. Dask isn't involved at all here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m = Prophet()\n",
    "m.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can make a forecast. Again, Dask isn't involved here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=365)\n",
    "forecast = m.predict(future)\n",
    "m.plot(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Diagnostics\n",
    "\n",
    "Prophet includes a `cross_validation` function method, which uses *simulated historical forecasts* to provide some idea of a model's quality.\n",
    "\n",
    "> This is done by selecting cutoff points in the history, and for each of them fitting the model using data only up to that cutoff point. We can then compare the forecasted values to the actual values.\n",
    "\n",
    "See https://facebook.github.io/prophet/docs/diagnostics.html for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import cross_validation, performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv = cross_validation(m, initial=\"730 days\", period=\"180 days\", horizon=\"365 days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, `cross_validation` determines some `cutoffs` based on the user's parameters to generate forecasts for. In this case, we ended up with 11. *The historical forecasts for each cutoff can be done entirely in parallel*. So in order to distribute cross validation, we'll just generate those cutoffs ourselves and call one `cross_validation` for each `cutoff`.\n",
    "\n",
    "Note that the rest of this example depends on `fbprophet>=0.7` (currently in development)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from distributed import Client, performance_report\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import generate_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = pd.Timedelta(\"730 days\")\n",
    "period = pd.Timedelta(\"180 days\")\n",
    "horizon = pd.Timedelta(\"365 days\")\n",
    "\n",
    "cutoffs = generate_cutoffs(\n",
    "    m.history.copy().reset_index(drop=True),\n",
    "    horizon=horizon,\n",
    "    initial=initial,\n",
    "    period=period,\n",
    ")\n",
    "cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `dask.delayed` and `fbprophet.diagnostics.single_cutoff_forecast` to lazily do all the cross validations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import single_cutoff_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_cv = dask.delayed(single_cutoff_forecast)\n",
    "\n",
    "# df2 is a somewhat large object.\n",
    "df2 = dask.delayed(m.history.copy().reset_index(drop=True))\n",
    "predict_columns = ['ds', 'yhat', 'yhat_lower', 'yhat_upper']\n",
    "\n",
    "\n",
    "cvs = [delayed_cv(df2, m, cutoff, horizon, predict_columns)\n",
    "       for cutoff in cutoffs]\n",
    "cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.scatter(df2);  # pre-scatter large data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finished instantly, since we haven't done anything yet.\n",
    "By passing those to `dask.compute` the cluster will get to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the distributed Dashboard here\n",
    "%time cvs = dask.compute(*cvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a list of DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can concatenate to get the same result as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv = pd.concat(cvs, ignore_index=True)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're back to the same result as if we had done things without Dask.\n",
    "We can compute `performance_metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import performance_metrics\n",
    "\n",
    "df_p = performance_metrics(df_cv)\n",
    "df_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot, e.g. the mean absolute percent error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.plot import plot_cross_validation_metric\n",
    "\n",
    "fig = plot_cross_validation_metric(df_cv, metric='mape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to improve\n",
    "\n",
    "Currently, this requires a bit of specialized knowledge to use correctly.\n",
    "It took me (a light user of Prophet) a bit of time to dig into Prophet's source code to determine the appropriate point of parallelization. And it takes a bit of Dask knowledge to know just what should be wrapped in `dask.delayed`.\n",
    "\n",
    "Currently, Prophet supports parallel evaluation in `cross_validation` with `multiprocessing`. It would be great if Prophet could do things using an interface like `concurrent.futures`. Then users could choose their own parallelism.\n",
    "\n",
    "\n",
    "```python\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# Parallelize with Threads\n",
    "cross_validation(model, executor=ThreadPoolExecutor())\n",
    "\n",
    "# Parallelize with Dask\n",
    "cross_validation(model, executor=client)\n",
    "```\n",
    "\n",
    "However, there are a few issues that would need to be solved upstream in Python itself (see https://github.com/dask/distributed/issues/3695) for more. For now, this post serves as a reference for a way to achieve parallel, distributed diagnostics usings Prophet's already great API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
